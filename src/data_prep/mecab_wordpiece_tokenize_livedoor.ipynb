{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import csv\n",
    "import sentencepiece as sp\n",
    "import os\n",
    "import collections\n",
    "import unicodedata\n",
    "from transformers import WordpieceTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.path.join(os.getcwd(),os.pardir,os.pardir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeCabSentenceSplitter(object):\n",
    "    def __init__(self, mecab_dict_path='/usr/local/lib/mecab/dic/mecab-ipadic-neologd'):\n",
    "        if mecab_dict_path is not None:\n",
    "            self.mecab = MeCab.Tagger('-d {} -O wakati'.format(mecab_dict_path))\n",
    "        else:\n",
    "            self.mecab = MeCab.Tagger('-O wakati')\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.mecab.parse(text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_livedoor(path):\n",
    "    all_labels = ['dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'smax', 'sports-watch', 'topic-news']\n",
    "    data_ = list(csv.reader(open(path,encoding='utf8'),delimiter='\\t',quotechar=None))\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for l in data_[1:]:\n",
    "        texts.append(l[0])\n",
    "        labels.append(all_labels.index(l[1]))\n",
    "    return texts,labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "livedoor_path = os.path.join(root_dir,'data/livedoor')\n",
    "train_path = os.path.join(livedoor_path,'train.tsv')\n",
    "val_path = os.path.join(livedoor_path,'dev.tsv')\n",
    "test_path = os.path.join(livedoor_path,'test.tsv')\n",
    "\n",
    "train_texts, train_labels = read_livedoor(train_path)\n",
    "val_texts, val_labels = read_livedoor(val_path)\n",
    "test_texts, test_labels = read_livedoor(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, encoding='utf8') as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "def pre_processing(sent_splitter,line,vocab_file,max_seq_len=512):\n",
    "    line = unicodedata.normalize('NFKC', line).replace(' ','')\n",
    "    \n",
    "    tokens = sent_splitter(line).lower().split()\n",
    "    \n",
    "    vocab_index = load_vocab(vocab_file)\n",
    "    tokenizer = WordpieceTokenizer(vocab=vocab_index, unk_token='[UNK]')\n",
    "    tokens = [sub_token for token in tokens for sub_token in tokenizer.tokenize(token)]\n",
    "    \n",
    "    if len(tokens)>max_seq_len-2:\n",
    "        tokens = tokens[:max_seq_len-2]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(root_dir,'model/namco_distilbert/vocab-lower.txt')\n",
    "sent_splitter = MeCabSentenceSplitter()\n",
    "\n",
    "tokenized_train_texts = []\n",
    "for text in tqdm(train_texts):\n",
    "    tokenized_train_texts.append(pre_processing(sent_splitter,text,vocab_file))\n",
    "assert len(tokenized_train_texts)==len(train_texts)\n",
    "\n",
    "train_output_path = os.path.join(livedoor_path,'namco_train_tokenized.tsv')\n",
    "with open(train_output_path,'w',encoding='utf8') as train_output:\n",
    "    for i in range(len(tokenized_train_texts)):\n",
    "        train_output.write(f'{tokenized_train_texts[i]}\\t{train_labels[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(root_dir,'model/namco_distilbert/vocab-lower.txt')\n",
    "sent_splitter = MeCabSentenceSplitter()\n",
    "\n",
    "tokenized_val_texts = []\n",
    "for text in tqdm(val_texts):\n",
    "    tokenized_val_texts.append(pre_processing(sent_splitter,text,vocab_file))\n",
    "assert len(tokenized_val_texts)==len(val_texts)\n",
    "\n",
    "val_output_path = os.path.join(livedoor_path,'namco_dev_tokenized.tsv')\n",
    "with open(val_output_path,'w',encoding='utf8') as val_output:\n",
    "    for i in range(len(tokenized_val_texts)):\n",
    "        val_output.write(f'{tokenized_val_texts[i]}\\t{val_labels[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(root_dir,'model/namco_distilbert/vocab-lower.txt')\n",
    "sent_splitter = MeCabSentenceSplitter()\n",
    "\n",
    "tokenized_test_texts = []\n",
    "for text in tqdm(test_texts):\n",
    "    tokenized_test_texts.append(pre_processing(sent_splitter,text,vocab_file))\n",
    "assert len(tokenized_test_texts)==len(test_texts)\n",
    "\n",
    "test_output_path = os.path.join(livedoor_path,'namco_test_tokenized.tsv')\n",
    "with open(test_output_path,'w',encoding='utf8') as test_output:\n",
    "    for i in range(len(tokenized_test_texts)):\n",
    "        test_output.write(f'{tokenized_test_texts[i]}\\t{test_labels[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
