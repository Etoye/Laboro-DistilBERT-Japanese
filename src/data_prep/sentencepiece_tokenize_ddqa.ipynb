{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as sp\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.path.join(os.getcwd(),os.pardir,os.pardir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(root_dir,'model/laboro_distilbert/tokenizer/ccc_13g_unigram.model')\n",
    "tokenizer_sp = sp.SentencePieceProcessor(model_file=model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqa_copy_format(ori_dic):\n",
    "    output_dic = {'version':ori_dic['version'],'data':[]}\n",
    "    data_dic = {'title':ori_dic['data'][0]['title'],'paragraphs':[]}\n",
    "    output_dic['data'].append(data_dic)\n",
    "    return output_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqa_path = os.path.join(root_dir,'data/ddqa/RC-QA')\n",
    "train_path = os.path.join(ddqa_path,'DDQA-1.0_RC-QA_train.json')\n",
    "val_path = os.path.join(ddqa_path,'DDQA-1.0_RC-QA_dev.json')\n",
    "test_path = os.path.join(ddqa_path,'DDQA-1.0_RC-QA_test.json')\n",
    "\n",
    "train_ori = json.load(open(train_path,encoding='utf8'))\n",
    "val_ori = json.load(open(val_path,encoding='utf8'))\n",
    "test_ori = json.load(open(test_path,encoding='utf8'))\n",
    "\n",
    "train_output = ddqa_copy_format(train_ori)\n",
    "val_output = ddqa_copy_format(val_ori)\n",
    "test_output = ddqa_copy_format(test_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_context(tokenizer_sp,line):\n",
    "    ids = tokenizer_sp.encode(line, out_type=int)\n",
    "    tokens = tokenizer_sp.id_to_piece(ids)\n",
    "    #print(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def pre_processing(tokenizer_sp,line,max_seq_len=512):\n",
    "    ids = tokenizer_sp.encode(line, out_type=int)\n",
    "    if len(ids)>max_seq_len-2:\n",
    "        ids = ids[:max_seq_len-2]\n",
    "    tokens = tokenizer_sp.id_to_piece(ids)\n",
    "    #print(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(ori_dic):\n",
    "    for para in ori_dic['data'][0]['paragraphs']:\n",
    "        yield para\n",
    "\n",
    "def tokenize_para(ori_para):\n",
    "    output_para = {'context':'','qas':[]}\n",
    "    \n",
    "    context = ori_para['context'].replace(\" \", \".\").replace(\"â€¦\", \".\")\n",
    "    output_context = pre_processing_context(tokenizer_sp,context)\n",
    "    output_para['context'] = output_context\n",
    "    \n",
    "    for qas in ori_para['qas']:\n",
    "        qas_dic_format = {'id':'','question':'','answers':[],'is_impossible':None}\n",
    "        qas_dic_format['id'] = qas['id']\n",
    "        qas_dic_format['is_impossible'] = qas['is_impossible']\n",
    "        \n",
    "        question =  qas['question']\n",
    "        tokenized_question = pre_processing(tokenizer_sp,question)\n",
    "        qas_dic_format['question'] = tokenized_question\n",
    "        \n",
    "        for answer in qas['answers']:\n",
    "            answers_dic_format = {'text':'','answer_start':-1}\n",
    "            answers_dic_format['answer_start'] = answer['answer_start']\n",
    "            \n",
    "            text = answer['text']\n",
    "            tokenized_text = pre_processing(tokenizer_sp,text)\n",
    "            answers_dic_format['text'] = tokenized_text\n",
    "            \n",
    "            qas_dic_format['answers'].append(answers_dic_format)\n",
    "        \n",
    "        output_para['qas'].append(qas_dic_format)\n",
    "        \n",
    "    return output_para\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in read_data(train_ori):\n",
    "    output_para = tokenize_para(para)\n",
    "    train_output['data'][0]['paragraphs'].append(output_para)\n",
    "\n",
    "train_output_path = os.path.join(ddqa_path,'tokenized_DDQA-1.0_RC-QA_train.json')\n",
    "json.dump(train_output,open(train_output_path,'w',encoding='utf8'),ensure_ascii=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in read_data(val_ori):\n",
    "    output_para = tokenize_para(para)\n",
    "    val_output['data'][0]['paragraphs'].append(output_para)\n",
    "\n",
    "val_output_path = os.path.join(ddqa_path,'tokenized_DDQA-1.0_RC-QA_dev.json')\n",
    "json.dump(val_output,open(val_output_path,'w',encoding='utf8'),ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for para in read_data(test_ori):\n",
    "    output_para = tokenize_para(para)\n",
    "    test_output['data'][0]['paragraphs'].append(output_para)\n",
    "\n",
    "test_output_path = os.path.join(ddqa_path,'tokenized_DDQA-1.0_RC-QA_test.json')\n",
    "json.dump(test_output,open(test_output_path,'w',encoding='utf8'),ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
