{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as sp\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.path.join(os.getcwd(),os.pardir,os.pardir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = os.path.join(root_dir,'model/laboro_distilbert/tokenizer/ccc_13g_unigram.model')\n",
    "tokenizer_sp = sp.SentencePieceProcessor(model_file=model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_livedoor(path):\n",
    "    all_labels = ['dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'smax', 'sports-watch', 'topic-news']\n",
    "    data_ = list(csv.reader(open(path,encoding='utf8'),delimiter='\\t',quotechar=None))\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for l in data_[1:]:\n",
    "        texts.append(l[0])\n",
    "        labels.append(all_labels.index(l[1]))\n",
    "    return texts,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livedoor_path = os.path.join(root_dir,'data/livedoor')\n",
    "train_path = os.path.join(livedoor_path,'train.tsv')\n",
    "val_path = os.path.join(livedoor_path,'dev.tsv')\n",
    "test_path = os.path.join(livedoor_path,'test.tsv')\n",
    "\n",
    "train_texts, train_labels = read_livedoor(train_path)\n",
    "val_texts, val_labels = read_livedoor(val_path)\n",
    "test_texts, test_labels = read_livedoor(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(tokenizer_sp,line,max_seq_len=512):\n",
    "    ids = tokenizer_sp.encode(line, out_type=int)\n",
    "    if len(ids)>max_seq_len-2:\n",
    "        ids = ids[:max_seq_len-2]\n",
    "    tokens = tokenizer_sp.id_to_piece(ids)\n",
    "    #print(tokens)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_texts = []\n",
    "for text in train_texts:\n",
    "    tokenized_train_texts.append(pre_processing(tokenizer_sp,text))\n",
    "assert len(tokenized_train_texts)==len(train_texts)\n",
    "\n",
    "train_output_path = os.path.join(livedoor_path,'train_tokenized.txt')\n",
    "with open(train_output_path,'w',encoding='utf8') as train_output:\n",
    "    for i in range(len(tokenized_train_texts)):\n",
    "        train_output.write(f'{tokenized_train_texts[i]}\\t{train_labels[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_val_texts = []\n",
    "for text in val_texts:\n",
    "    tokenized_val_texts.append(pre_processing(tokenizer_sp,text))\n",
    "assert len(tokenized_val_texts)==len(val_texts)\n",
    "\n",
    "val_output_path = os.path.join(livedoor_path,'dev_tokenized.txt')\n",
    "with open(val_output_path,'w',encoding='utf8') as val_output:\n",
    "    for i in range(len(tokenized_val_texts)):\n",
    "        val_output.write(f'{tokenized_val_texts[i]}\\t{val_labels[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_texts = []\n",
    "for text in test_texts:\n",
    "    tokenized_test_texts.append(pre_processing(tokenizer_sp,text))\n",
    "assert len(tokenized_test_texts)==len(test_texts)\n",
    "\n",
    "test_output_path = os.path.join(livedoor_path,'test_tokenized.txt')\n",
    "with open(test_output_path,'w',encoding='utf8') as test_output:\n",
    "    for i in range(len(tokenized_test_texts)):\n",
    "        test_output.write(f'{tokenized_test_texts[i]}\\t{test_labels[i]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
