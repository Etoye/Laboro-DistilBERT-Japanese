{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import json\n",
    "import sentencepiece as sp\n",
    "import os\n",
    "import collections\n",
    "import unicodedata\n",
    "from transformers import WordpieceTokenizer\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.path.join(os.getcwd(),os.pardir,os.pardir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeCabSentenceSplitter(object):\n",
    "    def __init__(self, mecab_dict_path='/usr/local/lib/mecab/dic/mecab-ipadic-neologd'):\n",
    "        if mecab_dict_path is not None:\n",
    "            self.mecab = MeCab.Tagger('-d {} -O wakati'.format(mecab_dict_path))\n",
    "        else:\n",
    "            self.mecab = MeCab.Tagger('-O wakati')\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.mecab.parse(text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddqa_copy_format(ori_dic):\n",
    "    output_dic = {'version':ori_dic['version'],'data':[]}\n",
    "    data_dic = {'title':ori_dic['data'][0]['title'],'paragraphs':[]}\n",
    "    output_dic['data'].append(data_dic)\n",
    "    return output_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddqa_path = os.path.join(root_dir,'data/ddqa/RC-QA')\n",
    "train_path = os.path.join(ddqa_path,'DDQA-1.0_RC-QA_train.json')\n",
    "val_path = os.path.join(ddqa_path,'DDQA-1.0_RC-QA_dev.json')\n",
    "test_path = os.path.join(ddqa_path,'DDQA-1.0_RC-QA_test.json')\n",
    "\n",
    "train_ori = json.load(open(train_path,encoding='utf8'))\n",
    "val_ori = json.load(open(val_path,encoding='utf8'))\n",
    "test_ori = json.load(open(test_path,encoding='utf8'))\n",
    "\n",
    "train_output = ddqa_copy_format(train_ori)\n",
    "val_output = ddqa_copy_format(val_ori)\n",
    "test_output = ddqa_copy_format(test_ori)\n",
    "\n",
    "train_norm = ddqa_copy_format(train_ori)\n",
    "val_norm = ddqa_copy_format(val_ori)\n",
    "test_norm = ddqa_copy_format(test_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, encoding='utf8') as reader:\n",
    "        while True:\n",
    "            token = reader.readline()\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "def pre_processing_context(sent_splitter,line,vocab_file):\n",
    "    tokens = sent_splitter(line).lower().split()\n",
    "    \n",
    "    vocab_index = load_vocab(vocab_file)\n",
    "    tokenizer = WordpieceTokenizer(vocab=vocab_index, unk_token='[UNK]')\n",
    "    tokens = [sub_token for token in tokens for sub_token in tokenizer.tokenize(token)]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def pre_processing(sent_splitter,line,vocab_file,max_seq_len=512):\n",
    "    tokens = sent_splitter(line).lower().split()\n",
    "    \n",
    "    vocab_index = load_vocab(vocab_file)\n",
    "    tokenizer = WordpieceTokenizer(vocab=vocab_index, unk_token='[UNK]')\n",
    "    tokens = [sub_token for token in tokens for sub_token in tokenizer.tokenize(token)]\n",
    "    \n",
    "    if len(tokens)>max_seq_len-2:\n",
    "        tokens = tokens[:max_seq_len-2]\n",
    "    \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(ori_dic):\n",
    "    for para in ori_dic['data'][0]['paragraphs']:\n",
    "        yield para\n",
    "        \n",
    "def read_data_test(ori_dic):\n",
    "    for para in ori_dic['data'][0]['paragraphs'][:1]:\n",
    "        yield para\n",
    "\n",
    "def tokenize_para(ori_para,vocab_file,sent_splitter):\n",
    "    output_para = {'context':'','qas':[]}\n",
    "    \n",
    "    context = ori_para['context']\n",
    "    output_context = pre_processing_context(sent_splitter,context,vocab_file)\n",
    "    output_para['context'] = output_context\n",
    "    \n",
    "    for qas in ori_para['qas']:\n",
    "        qas_dic_format = {'id':'','question':'','answers':[],'is_impossible':None}\n",
    "        qas_dic_format['id'] = qas['id']\n",
    "        qas_dic_format['is_impossible'] = qas['is_impossible']\n",
    "        \n",
    "        question =  qas['question']\n",
    "        tokenized_question = pre_processing(sent_splitter,question,vocab_file)\n",
    "        qas_dic_format['question'] = tokenized_question\n",
    "        \n",
    "        for answer in qas['answers']:\n",
    "            answers_dic_format = {'text':'','answer_start':-1}\n",
    "            answers_dic_format['answer_start'] = answer['answer_start']\n",
    "            \n",
    "            text = answer['text']\n",
    "            tokenized_text = pre_processing(sent_splitter,text,vocab_file)\n",
    "            answers_dic_format['text'] = tokenized_text\n",
    "            \n",
    "            qas_dic_format['answers'].append(answers_dic_format)\n",
    "        \n",
    "        output_para['qas'].append(qas_dic_format)\n",
    "        \n",
    "    return output_para\n",
    "\n",
    "def normalize_para(ori_para):\n",
    "    norm_para = {'context':'','qas':[]}\n",
    "    \n",
    "    context = ori_para['context'].replace(\" \", \".\").replace(\"…\",\".\").replace('‥','.')\n",
    "    norm_context = unicodedata.normalize('NFKC', context).replace(' ','')\n",
    "    norm_para['context'] = norm_context\n",
    "    \n",
    "    for qas in ori_para['qas']:\n",
    "        qas_dic_format = {'id':'','question':'','answers':[],'is_impossible':None}\n",
    "        qas_dic_format['id'] = qas['id']\n",
    "        qas_dic_format['is_impossible'] = qas['is_impossible']\n",
    "        \n",
    "        question = qas['question'].replace(\" \", \".\").replace(\"…\",\".\").replace('‥','.')\n",
    "        norm_question = unicodedata.normalize('NFKC', question).replace(' ','')\n",
    "        qas_dic_format['question'] = norm_question\n",
    "        \n",
    "        for answer in qas['answers']:\n",
    "            answers_dic_format = {'text':'','answer_start':-1}\n",
    "            answers_dic_format['answer_start'] = answer['answer_start']\n",
    "            \n",
    "            text = answer['text'].replace(\" \", \".\").replace(\"…\",\".\").replace('‥','.')\n",
    "            norm_text = unicodedata.normalize('NFKC', text).replace(' ','')\n",
    "            answers_dic_format['text'] = norm_text\n",
    "            \n",
    "            qas_dic_format['answers'].append(answers_dic_format)\n",
    "        \n",
    "        norm_para['qas'].append(qas_dic_format)\n",
    "        \n",
    "    return norm_para\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(root_dir,'model/namco_distilbert/vocab-lower.txt')\n",
    "sent_splitter = MeCabSentenceSplitter()\n",
    "\n",
    "train_output = ddqa_copy_format(train_ori)\n",
    "train_norm = ddqa_copy_format(train_ori)\n",
    "\n",
    "for para in tqdm(read_data(train_ori)):\n",
    "    norm_para = normalize_para(para)\n",
    "    train_norm['data'][0]['paragraphs'].append(norm_para)\n",
    "    \n",
    "    output_para = tokenize_para(norm_para,vocab_file,sent_splitter)\n",
    "    train_output['data'][0]['paragraphs'].append(output_para)\n",
    "\n",
    "train_norm_path = os.path.join(ddqa_path,'namco_normalized_DDQA-1.0_RC-QA_train.json')\n",
    "json.dump(train_norm,open(train_norm_path,'w',encoding='utf8'),ensure_ascii=False)\n",
    "\n",
    "train_output_path = os.path.join(ddqa_path,'namco_tokenized_DDQA-1.0_RC-QA_train.json')\n",
    "json.dump(train_output,open(train_output_path,'w',encoding='utf8'),ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(root_dir,'model/namco_distilbert/vocab-lower.txt')\n",
    "sent_splitter = MeCabSentenceSplitter()\n",
    "\n",
    "val_output = ddqa_copy_format(val_ori)\n",
    "val_norm = ddqa_copy_format(val_ori)\n",
    "\n",
    "for para in tqdm(read_data(val_ori)):\n",
    "    norm_para = normalize_para(para)\n",
    "    val_norm['data'][0]['paragraphs'].append(norm_para)\n",
    "    \n",
    "    output_para = tokenize_para(norm_para,vocab_file,sent_splitter)\n",
    "    val_output['data'][0]['paragraphs'].append(output_para)\n",
    "\n",
    "val_norm_path = os.path.join(ddqa_path,'namco_normalized_DDQA-1.0_RC-QA_dev.json')\n",
    "json.dump(val_norm,open(val_norm_path,'w',encoding='utf8'),ensure_ascii=False)  \n",
    "\n",
    "val_output_path = os.path.join(ddqa_path,'namco_tokenized_DDQA-1.0_RC-QA_dev.json')\n",
    "json.dump(val_output,open(val_output_path,'w',encoding='utf8'),ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_file = os.path.join(root_dir,'model/namco_distilbert/vocab-lower.txt')\n",
    "sent_splitter = MeCabSentenceSplitter()\n",
    "\n",
    "test_output = ddqa_copy_format(test_ori)\n",
    "test_norm = ddqa_copy_format(test_ori)\n",
    "\n",
    "for para in tqdm(read_data(test_ori)):\n",
    "    norm_para = normalize_para(para)\n",
    "    test_norm['data'][0]['paragraphs'].append(norm_para)\n",
    "    \n",
    "    output_para = tokenize_para(norm_para,vocab_file,sent_splitter)\n",
    "    test_output['data'][0]['paragraphs'].append(output_para)\n",
    "\n",
    "test_norm_path = os.path.join(ddqa_path,'namco_ normalized_DDQA-1.0_RC-QA_test.json')\n",
    "json.dump(test_norm,open(test_norm_path,'w',encoding='utf8'),ensure_ascii=False)\n",
    "    \n",
    "test_output_path = os.path.join(ddqa_path,'namco_tokenized_DDQA-1.0_RC-QA_test.json')\n",
    "json.dump(test_output,open(test_output_path,'w',encoding='utf8'),ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
